{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  163k  100  163k    0     0   136k      0  0:00:01  0:00:01 --:--:--  136k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  954k  100  954k    0     0   251k      0  0:00:03  0:00:03 --:--:--  251k\n",
      "Archive:  ./data/blogcatalog/b.zip\n",
      "   creating: ./data/blogcatalog/BlogCatalog-dataset/\n",
      "   creating: ./data/blogcatalog/BlogCatalog-dataset/data/\n",
      "  inflating: ./data/blogcatalog/BlogCatalog-dataset/data/edges.csv  \n",
      "  inflating: ./data/blogcatalog/BlogCatalog-dataset/data/group-edges.csv  \n",
      "  inflating: ./data/blogcatalog/BlogCatalog-dataset/data/groups.csv  \n",
      "  inflating: ./data/blogcatalog/BlogCatalog-dataset/data/nodes.csv  \n",
      "  inflating: ./data/blogcatalog/BlogCatalog-dataset/readme.txt  \n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  350k  100  350k    0     0   256k      0  0:00:01  0:00:01 --:--:--  256k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  213k  100  213k    0     0   114k      0  0:00:01  0:00:01 --:--:--  114k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.0M  100 10.0M    0     0   678k      0  0:00:15  0:00:15 --:--:-- 1578k\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('./data'):\n",
    "    !mkdir ./data\n",
    "\n",
    "if not os.path.isdir('./data/cora'):\n",
    "    !mkdir ./data/cora\n",
    "    !curl -o ./data/cora/cora.tar.gz \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\"\n",
    "    !gzip -d < ./data/cora/cora.tar.gz | tar xf - --directory ./data\n",
    "    !rm ./data/cora/cora.tar.gz\n",
    "    \n",
    "if not os.path.isdir('./data/blogcatalog'):\n",
    "    !mkdir ./data/blogcatalog\n",
    "    !curl -o ./data/blogcatalog/b.zip \"http://socialcomputing.asu.edu/uploads/1283153973/BlogCatalog-dataset.zip\"\n",
    "    !unzip -d ./data/blogcatalog ./data/blogcatalog/b.zip\n",
    "    !mv ./data/blogcatalog/BlogCatalog-dataset/data/* ./data/blogcatalog/\n",
    "    !mv ./data/blogcatalog/BlogCatalog-dataset/readme.txt ./data/blogcatalog/readme.txt\n",
    "    # replace ',' by ' ' as delimiter\n",
    "    !sed -i 's/,/ /g' ./data/blogcatalog/edges.csv\n",
    "    !sed -i 's/,/ /g' ./data/blogcatalog/group-edges.csv\n",
    "    !rm ./data/blogcatalog/b.zip\n",
    "    !rm -r ./data/blogcatalog/BlogCatalog-dataset/    \n",
    "    \n",
    "if not os.path.isdir('./data/citeseer'):\n",
    "    !mkdir ./data/citeseer\n",
    "    !curl -o ./data/citeseer/c.tgz \"https://linqs-data.soe.ucsc.edu/public/lbc/citeseer.tgz\"\n",
    "    !gzip -d < ./data/citeseer/c.tgz | tar xf - --directory ./data\n",
    "    !rm ./data/citeseer/c.tgz\n",
    "    \n",
    "# https://snap.stanford.edu/data/egonets-Facebook.html\n",
    "if not os.path.isdir('./data/facebook'):\n",
    "    !mkdir ./data/facebook\n",
    "    !curl -o ./data/facebook/facebook.gz \"https://snap.stanford.edu/data/facebook_combined.txt.gz\"\n",
    "    !gzip -d ./data/facebook/facebook.gz\n",
    "    !mv ./data/facebook/facebook ./data/facebook/facebook.edges\n",
    "    \n",
    "# https://snap.stanford.edu/data/com-Youtube.html\n",
    "if not os.path.isdir('./data/youtube'):\n",
    "    !mkdir ./data/youtube\n",
    "    !curl -o ./data/youtube/youtube.gz \"https://snap.stanford.edu/data/bigdata/communities/com-youtube.ungraph.txt.gz\"\n",
    "    !gzip -d ./data/youtube/youtube.gz\n",
    "    !mv ./data/youtube/youtube ./data/youtube/youtube.edges\n",
    "    # remove header (first 4 lines)\n",
    "    !sed -i '1,4d' ./data/youtube/youtube.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Datasets\n",
    "Replace node-IDs with own ids, starting at 0. Remove self-loops and isolates (only present in citeseer). Only keep edgelists and label assignments (document content not needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "if not os.path.isdir('./data/processed'):\n",
    "    !mkdir ./data/processed\n",
    "if not os.path.isdir('./data/emb'):\n",
    "    !mkdir ./data/emb\n",
    "    \n",
    "datapath = './data/'\n",
    "datasets = {\n",
    "    'cora':{'edges':'cora.cites','labels':'cora.content'},\n",
    "    'blogcatalog':{'edges':'edges.csv','labels':'group-edges.csv'},\n",
    "    'citeseer':{'edges':'citeseer.cites','labels':'citeseer.content'}\n",
    "}\n",
    "for d in datasets:\n",
    "    node2idx = dict()\n",
    "    g = nx.read_edgelist(datapath + d + '/' + datasets[d]['edges'])\n",
    "    g.remove_edges_from(g.selfloop_edges()) # remove self-loops\n",
    "    g.remove_nodes_from(list(nx.isolates(g))) # remove isolates\n",
    "    for n in g.nodes():\n",
    "        node2idx[n] = str(len(node2idx))\n",
    "    with open(datapath + 'processed/' + d + '.edges','w') as f:\n",
    "        for e in g.edges():\n",
    "            f.write(node2idx[e[0]] + '\\t' + node2idx[e[1]] + '\\n')\n",
    "    with open(datapath + d + '/' + datasets[d]['labels']) as f_in:\n",
    "        with open(datapath + 'processed/' + d + '.labels','w') as f_out:\n",
    "            for line in f_in.readlines():\n",
    "                line = line.split()\n",
    "                if line[0] in node2idx:\n",
    "                    f_out.write(node2idx[line[0]] + '\\t' + line[-1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['facebook','youtube']\n",
    "for d in datasets:\n",
    "    node2idx = dict()\n",
    "    g = nx.read_edgelist(datapath + d + '/' + d + '.edges')\n",
    "    g.remove_edges_from(g.selfloop_edges()) # remove self-loops\n",
    "    g.remove_nodes_from(list(nx.isolates(g))) # remove isolates\n",
    "    for n in g.nodes():\n",
    "        node2idx[n] = str(len(node2idx))\n",
    "    with open(datapath + 'processed/' + d + '.edges','w') as f:\n",
    "        for e in g.edges():\n",
    "            f.write(node2idx[e[0]] + '\\t' + node2idx[e[1]] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### cora ###\n",
      "## walking ##\n",
      "done 216640 of 216640\n",
      "# random #\n",
      "# deepwalk #\n",
      "# harp #\n",
      "coarsened to 18 levels\n",
      "# walklets #\n",
      "\n",
      "Optimization round: 1/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Optimization round: 2/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Models are integrated to be multi scale.\n",
      "Saving to disk.\n",
      "# halk #\n",
      "training 1 of 4\n",
      "training 2 of 4\n",
      "training 3 of 4\n",
      "training 4 of 4\n",
      "### citeseer ###\n",
      "## walking ##\n",
      "done 262320 of 262320\n",
      "# random #\n",
      "# deepwalk #\n",
      "# harp #\n",
      "coarsened to 17 levels\n",
      "# walklets #\n",
      "\n",
      "Optimization round: 1/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Optimization round: 2/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Models are integrated to be multi scale.\n",
      "Saving to disk.\n",
      "# halk #\n",
      "training 1 of 4\n",
      "training 2 of 4\n",
      "training 3 of 4\n",
      "training 4 of 4\n",
      "### blogcatalog ###\n",
      "## walking ##\n",
      "done 824960 of 824960\n",
      "# random #\n",
      "# deepwalk #\n",
      "# harp #\n",
      "coarsened to 24 levels\n",
      "# walklets #\n",
      "\n",
      "Optimization round: 1/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Optimization round: 2/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Models are integrated to be multi scale.\n",
      "Saving to disk.\n",
      "# halk #\n",
      "training 1 of 4\n",
      "training 2 of 4\n",
      "training 3 of 4\n",
      "training 4 of 4\n",
      "### youtube ###\n",
      "## walking ##\n",
      "done 11348900 of 11348900\n",
      "# random #\n",
      "# deepwalk #\n",
      "# harp #\n",
      "coarsened to 43 levels\n",
      "# walklets #\n",
      "\n",
      "Optimization round: 1/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Optimization round: 2/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Models are integrated to be multi scale.\n",
      "Saving to disk.\n",
      "# halk #\n",
      "training 1 of 4\n",
      "training 2 of 4\n",
      "training 3 of 4\n",
      "training 4 of 4\n",
      "### facebook ###\n",
      "## walking ##\n",
      "done 40390 of 40390\n",
      "# random #\n",
      "# deepwalk #\n",
      "# harp #\n",
      "coarsened to 17 levels\n",
      "# walklets #\n",
      "\n",
      "Optimization round: 1/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Optimization round: 2/2.\n",
      "Creating documents.\n",
      "Fitting model.\n",
      "\n",
      "Models are integrated to be multi scale.\n",
      "Saving to disk.\n",
      "# halk #\n",
      "training 1 of 4\n",
      "training 2 of 4\n",
      "training 3 of 4\n",
      "training 4 of 4\n",
      "CPU times: user 19d 13h 26min 41s, sys: 1h 42min 11s, total: 19d 15h 8min 53s\n",
      "Wall time: 1d 10h 8min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from deepwalk import graph\n",
    "import halk\n",
    "from gensim.models import Word2Vec\n",
    "import walklets\n",
    "import node2vec\n",
    "import networkx as nx\n",
    "from HP import harp\n",
    "import numpy as np\n",
    "\n",
    "data_path = './data/processed/'\n",
    "emb_path = './data/emb/'\n",
    "# loop over datasets\n",
    "datasets = ['cora','citeseer','blogcatalog','youtube','facebook']\n",
    "num_paths = 80 # reproduction {walklets:1000,harp:40,n2v:10}\n",
    "path_length = 40 # reproduction {walklets:11,harp:10,n2v:80}\n",
    "dim = 128\n",
    "workers = 18\n",
    "window=10 \n",
    "epochs=5\n",
    "negative=5\n",
    "sample=0.1\n",
    "min_count=0\n",
    "min_alpha=0.001 # reproduction {deepwalk:0.0001,n2v:0.0001}\n",
    "alpha=0.025\n",
    "\n",
    "for d in datasets:\n",
    "    if d == 'youtube':\n",
    "        num_paths = 10\n",
    "        path_length = 80\n",
    "    print(\"###\", d, \"###\")\n",
    "    print(\"## walking ##\")\n",
    "    # deepwalk random walk generation (same as n2v with p=q=1)\n",
    "    G = graph.load_edgelist(data_path + d + '.edges')\n",
    "    walks = graph.build_deepwalk_corpus(G, num_paths=num_paths, path_length=path_length) \n",
    "    print(\"done\")\n",
    "    \n",
    "#     # n2v random walk generation (slower due to pre-processing of transition probs)\n",
    "#     G = nx.read_edgelist(data_path + d + '.edges')\n",
    "#     for edge in G.edges():\n",
    "#             G[edge[0]][edge[1]]['weight'] = 1\n",
    "#     nG = node2vec.Graph(G, False, 0.25, 0.25)\n",
    "#     print('preprocessing')\n",
    "#     nG.preprocess_transition_probs()\n",
    "#     print('walking')\n",
    "#     walks = nG.simulate_walks(num_paths, path_length)\n",
    "\n",
    "    # random (initialize embeddings randomly, don't train)\n",
    "    print(\"# random #\")\n",
    "    rand_model = Word2Vec(size=dim,min_count=0)\n",
    "    rand_model.build_vocab(walks)\n",
    "    rand_model.save_word2vec_format(emb_path + d + '.random.wv')\n",
    "    \n",
    "    # deepwalk\n",
    "    print(\"# deepwalk #\")\n",
    "    dw = Word2Vec(walks, sg=1, size=dim, workers=workers, window=window, negative=negative, sample=sample, \n",
    "                  min_count=min_count, min_alpha=min_alpha, alpha=alpha, iter=epochs)\n",
    "    dw.save_word2vec_format(emb_path + d + '.deepwalk.wv')\n",
    "    \n",
    "#     # node2vec\n",
    "#     print(\"node2vec\")\n",
    "#     dw = Word2Vec(walks, sg=1, size=dim, workers=workers, window=window, negative=negative, sample=sample, \n",
    "#                   min_count=min_count, min_alpha=min_alpha, alpha=alpha, iter=epochs)\n",
    "#     dw.save_word2vec_format(emb_path + d + '.node2vec.wv')\n",
    "    \n",
    "    # HARP\n",
    "    print(\"# harp #\")\n",
    "    hp_graph = harp.load_graph(data_path + d + '.edges')\n",
    "    harp.dw(hp_graph,iter_count=1,representation_size=dim, sfdp_path='./HP/SFDP/sfdp_linux', \n",
    "            num_walks=num_paths, walk_length=path_length,window_size=window,\n",
    "           outfile=emb_path + d + '.harp.wv',workers=workers,hs=0,sg=1,\n",
    "            negative=negative,\n",
    "            sample=sample,\n",
    "            min_count=min_count,\n",
    "            min_alpha=min_alpha,\n",
    "            alpha=alpha)\n",
    "\n",
    "    # walklets\n",
    "    print(\"# walklets #\")\n",
    "    walker = walklets.WalkletMachine(walklets.AttrDict({\n",
    "        'input':data_path + d + '.edges',\n",
    "        'output':emb_path + d + '.walklets.wv',\n",
    "        'window_size':2, # skip-window size, not window size\n",
    "        'dimensions':dim,\n",
    "        'walk_number':num_paths,\n",
    "        'walk_length':path_length,\n",
    "        'workers':workers,\n",
    "        'window':window,\n",
    "        'negative':negative,\n",
    "        'sample':sample,\n",
    "        'min_count':min_count,\n",
    "        'min_alpha':min_alpha,\n",
    "        'alpha':alpha,\n",
    "        'iter':epochs\n",
    "    }))\n",
    "    walker.walks = walks # re-use already created walks instead of walking again\n",
    "    walker.create_embedding()\n",
    "    walker.save_model()\n",
    "\n",
    "    # HALK\n",
    "    print(\"# halk #\")\n",
    "    percentages = [0.1,0.2,0.4,1]\n",
    "    start_alphas = [0.025 for i in range(4)]\n",
    "    end_alphas = [0.001 for i in range(4)]\n",
    "    epochs_list = [10,5,3,1]\n",
    "    halk_model = halk.embed(walks, percentages, start_alphas, end_alphas, epochs_list, dimensions=dim, workers=workers, \n",
    "                            window=window, negative=negative, sample=sample, min_count=min_count, shuffle=False)\n",
    "    halk_model.save_word2vec_format(emb_path + d + '.halk.wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### cora ###\n",
      "halk  Macro F1:  0.816499559297 0.0217438766737\n",
      "harp  Macro F1:  0.813318212421 0.0249751491136\n",
      "walklets  Macro F1:  0.810979845501 0.0245962082751\n",
      "deepwalk  Macro F1:  0.813328141553 0.0197573656122\n",
      "### citeseer ###\n",
      "halk  Macro F1:  0.569702103703 0.0324652377252\n",
      "harp  Macro F1:  0.561647249213 0.0202259437483\n",
      "walklets  Macro F1:  0.557359332314 0.0285476271785\n",
      "deepwalk  Macro F1:  0.555448282517 0.0281922118646\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "data_path = './data/processed/'\n",
    "emb_path = './data/emb/'\n",
    "datasets = ['cora','citeseer']\n",
    "algos = ['halk','harp','walklets','deepwalk']\n",
    "folds = 10\n",
    "result = {a:{} for a in algos}\n",
    "\n",
    "for d in datasets:\n",
    "    print('### ' + d + ' ###')\n",
    "    labels = defaultdict(str)\n",
    "    with open(data_path + d + '.labels') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split()\n",
    "            labels[int(line[0])] = line[1]\n",
    "    y = [labels[i] for i in labels]\n",
    "    X_idx = [i for i in labels]\n",
    "    for algo in algos:\n",
    "        model = Word2Vec.load_word2vec_format(emb_path + d + '.' + algo + '.wv',binary=False)\n",
    "        X = [model[str(i)] for i in X_idx]\n",
    "        clf = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "#         clf = SVC(kernel='linear')\n",
    "        scores = []\n",
    "        for i in range(folds):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=i)\n",
    "            clf.fit(X_train,y_train)\n",
    "            score = metrics.f1_score(y_test,clf.predict(X_test), average='macro')\n",
    "            scores.append(score)\n",
    "        print(algo, ' Macro F1: ', np.mean(scores), np.std(scores))  \n",
    "        result[algo][d] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlogCatalog (multi-label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle providing the number of classes to predict (often used in the multiclass-evaluations for GE)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        assert len(X) == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halk  Macro F1:  0.276973558876 std 0.00378331742802\n",
      "harp  Macro F1:  0.276454758688 std 0.00587755926758\n",
      "walklets  Macro F1:  0.273101922431 std 0.00498511241567\n",
      "deepwalk  Macro F1:  0.275659316928 std 0.00509703795037\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "labels = defaultdict(list)\n",
    "with open(data_path + 'blogcatalog.labels') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        labels[int(line[0])].append(int(line[1].strip())-1)\n",
    "        \n",
    "y = [labels[i] for i in labels]\n",
    "X_idx = [i for i in labels]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y)\n",
    "\n",
    "for algo in algos:\n",
    "    model = Word2Vec.load_word2vec_format(emb_path + 'blogcatalog.' + algo + '.wv',binary=False)\n",
    "    X = [model[str(x)] for x in X_idx]\n",
    "    clf = TopKRanker(LogisticRegression(solver='liblinear'))\n",
    "    clf.classes_ = mlb.classes_\n",
    "    scores = []\n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=i)\n",
    "        y_train = mlb.transform(y_train)\n",
    "        top_k = [len(l) for l in y_test]\n",
    "        clf.fit(X_train,y_train)\n",
    "        pred = clf.predict(X_test, top_k)\n",
    "        score = metrics.f1_score(mlb.transform(y_test),mlb.transform(pred), average='macro')\n",
    "        scores.append(score)\n",
    "    print(algo, ' Macro F1: ', np.mean(scores),'std',np.std(scores))  \n",
    "    result[algo]['blogcatalog'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halk 0.554391740625\n",
      "harp 0.550473406774\n",
      "walklets 0.547147033415\n",
      "deepwalk 0.548145246999\n"
     ]
    }
   ],
   "source": [
    "for algo in result:\n",
    "    print(algo, np.mean(list(result[algo].values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### blogcatalog###\n",
      "Regenerating link prediction graphs\n",
      "Read graph, nodes: 10312, edges: 333983\n",
      "Finding 166991 of 52829533 non-edges\n",
      "Finding 166991 positive edges of 333983 total edges\n",
      "Found: 166990    \n",
      "Read graph, nodes: 10312, edges: 333983\n",
      "Finding 166991 of 52829533 non-edges\n",
      "Finding 166991 positive edges of 333983 total edges\n",
      "Found: 166990    \n",
      "## halk##\n",
      "{'hadamard': [0.82145006842727375], 'average': [0.93543304252414039], 'l1': [0.94900407653141827], 'l2': [0.9529188042503911]}\n",
      "## harp##\n",
      "{'hadamard': [0.78985976002940261], 'average': [0.93992224741446528], 'l1': [0.97485697633860879], 'l2': [0.97726696684512571]}\n",
      "## walklets##\n",
      "{'hadamard': [0.83649043811184476], 'average': [0.91279569597065624], 'l1': [0.98425703459862968], 'l2': [0.98544421458238207]}\n",
      "## deepwalk##\n",
      "{'hadamard': [0.80547420395904079], 'average': [0.9435172953696791], 'l1': [0.97858692068622199], 'l2': [0.98051736597870942]}\n",
      "### facebook###\n",
      "Regenerating link prediction graphs\n",
      "Read graph, nodes: 4039, edges: 88234\n",
      "Finding 44117 of 8066507 non-edges\n",
      "Finding 44117 positive edges of 88234 total edges\n",
      "Found: 44116    \n",
      "Read graph, nodes: 4039, edges: 88234\n",
      "Finding 44117 of 8066507 non-edges\n",
      "Finding 44117 positive edges of 88234 total edges\n",
      "Found: 44116    \n",
      "## halk##\n",
      "{'hadamard': [0.99080540671346351], 'average': [0.77957719605227738], 'l1': [0.98563650730508168], 'l2': [0.98583545560307806]}\n",
      "## harp##\n",
      "{'hadamard': [0.98778269145224395], 'average': [0.75528972768731883], 'l1': [0.9932472786451817], 'l2': [0.99355104941883676]}\n",
      "## walklets##\n",
      "{'hadamard': [0.98712065395261994], 'average': [0.76710421185186839], 'l1': [0.99434064729664906], 'l2': [0.99452749885581027]}\n",
      "## deepwalk##\n",
      "{'hadamard': [0.98830368048381034], 'average': [0.7654254656490076], 'l1': [0.99174151776007535], 'l2': [0.99187687956888138]}\n"
     ]
    }
   ],
   "source": [
    "from link_prediction import create_train_test_graphs\n",
    "from link_prediction import test_edge_functions, default_params, edge_functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "for d in ['blogcatalog','facebook']:\n",
    "    print('### ' + d + '###')\n",
    "    Gtrain, Gtest = create_train_test_graphs(data_path + d + '.edges')\n",
    "    edges_train, labels_train = Gtrain.get_selected_edges()\n",
    "    edges_test, labels_test = Gtest.get_selected_edges()\n",
    "    for algo in algos:\n",
    "        print('## ' + algo + '##')\n",
    "        auc = test_edge_functions(Gtrain,Gtest,emb_size=dim,\n",
    "                                  model=Word2Vec.load_word2vec_format(emb_path + d + '.' + algo + '.wv',binary=False),\n",
    "                                 edges_train=edges_train,\n",
    "                                 edges_test=edges_test,\n",
    "                                 labels_train=labels_train,\n",
    "                                 labels_test=labels_test,\n",
    "                                 lin_clf=LogisticRegression(solver='liblinear'))\n",
    "        print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest Path Approximation\n",
    "## Create train/test (landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set - all nodes 1134890, remaining nodes after selecting training landmarks 1134885\n",
      "test set path pairs for 5 of 5 landmarks\n",
      "shortest path pairs for 2 of 2 landmarks\n",
      "11348844 training, 4539528 test pairs\n",
      "train set - all nodes 4039, remaining nodes after selecting training landmarks 3939\n",
      "test set path pairs for 100 of 100 landmarks\n",
      "shortest path pairs for 100 of 100 landmarks\n",
      "779302 training, 759210 test pairs\n"
     ]
    }
   ],
   "source": [
    "import landmarks as lm\n",
    "import pickle\n",
    "\n",
    "\n",
    "train,test = lm.generate_data('./data/processed/youtube.edges', k_train=5, k_test=2)\n",
    "with open('./data/train_youtube.pkl', 'wb') as f:\n",
    "    pickle.dump(train,f)\n",
    "    \n",
    "with open('./data/test_youtube.pkl', 'wb') as f:\n",
    "    pickle.dump(test,f)\n",
    "    \n",
    "train,test = lm.generate_data('./data/processed/facebook.edges', k_train=100, k_test=100)\n",
    "with open('./data/train_facebook.pkl', 'wb') as f:\n",
    "    pickle.dump(train,f)  \n",
    "with open('./data/test_facebook.pkl', 'wb') as f:\n",
    "    pickle.dump(test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### facebook###\n",
      "## halk##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.399209704825 \t 0.115247951975\n",
      "had \t 0.419025039185 \t 0.119112326033\n",
      "sub \t 0.906753072272 \t 0.30628479102\n",
      "avg \t 0.683299745788 \t 0.225088541284\n",
      "cc \t 0.683303697264 \t 0.225082661112\n",
      "base 2 \t 1.77639915175 \t\t 0.409245010948\n",
      "## harp##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.429975895997 \t 0.121595489923\n",
      "had \t 0.50913186075 \t 0.150667996997\n",
      "sub \t 0.906753072272 \t 0.30628479102\n",
      "avg \t 0.675947366341 \t 0.224864564732\n",
      "cc \t 0.676013224273 \t 0.224868321771\n",
      "base 3 \t 1.1094295386 \t\t 0.280382709847\n",
      "## walklets##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.416954465826 \t 0.118550366578\n",
      "had \t 0.648742772092 \t 0.184618525773\n",
      "sub \t 0.906753072272 \t 0.30628479102\n",
      "avg \t 0.668394778783 \t 0.221494263147\n",
      "cc \t 0.668390827307 \t 0.22149669989\n",
      "base 4 \t 0.906753072272 \t\t 0.30628479102\n",
      "## deepwalk##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.45820260534 \t 0.130860446416\n",
      "had \t 0.414625729377 \t 0.120915522464\n",
      "sub \t 0.906753072272 \t 0.30628479102\n",
      "avg \t 0.683452536189 \t 0.226944938379\n",
      "cc \t 0.683447267554 \t 0.226943787433\n",
      "base 5 \t 1.3789412679 \t\t 0.500903037681\n",
      "## random##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.938072470068 \t 0.30134187103\n",
      "had \t 0.906753072272 \t 0.30628479102\n",
      "sub \t 0.906753072272 \t 0.30628479102\n",
      "avg \t 0.945806825516 \t 0.294011886416\n",
      "cc \t 0.945804191199 \t 0.294009054525\n",
      "base 6 \t 2.26081848237 \t\t 0.777459088112\n",
      "### youtube###\n",
      "## halk##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.837321853726 \t 0.175233603165\n",
      "had \t 0.787835651636 \t 0.161614608865\n",
      "sub \t 1.04876145714 \t 0.22844984637\n",
      "avg \t 0.541140841074 \t 0.112826201158\n",
      "cc \t 0.541273013406 \t 0.112830798088\n",
      "base 2 \t 3.16815889229 \t\t 0.600038922857\n",
      "## harp##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.773827146787 \t 0.162290639463\n",
      "had \t 0.679145937639 \t 0.136834711437\n",
      "sub \t 1.04876145714 \t 0.22844984637\n",
      "avg \t 0.503998653605 \t 0.0999421678648\n",
      "cc \t 0.490604089236 \t 0.098548513816\n",
      "base 3 \t 2.17013090348 \t\t 0.401044389878\n",
      "## walklets##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.788644546305 \t 0.166071255129\n",
      "had \t 1.06864458155 \t 0.231736790822\n",
      "sub \t 1.04876145714 \t 0.22844984637\n",
      "avg \t 0.461128557859 \t 0.094481160299\n",
      "cc \t 0.461479475399 \t 0.0946506771344\n",
      "base 4 \t 1.1817268227 \t\t 0.205257826246\n",
      "## deepwalk##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 0.992750788188 \t 0.208102619467\n",
      "had \t 0.668648370491 \t 0.138876819804\n",
      "sub \t 1.04876145714 \t 0.22844984637\n",
      "avg \t 0.455711695137 \t 0.0905546887252\n",
      "cc \t 0.455036735097 \t 0.0904462140058\n",
      "base 5 \t 0.653288623839 \t\t 0.124462733092\n",
      "## random##\n",
      "\t MAE \t\t\t MRE\n",
      "abs \t 1.00129969459 \t 0.214887911854\n",
      "had \t 1.04876145714 \t 0.22844984637\n",
      "sub \t 1.04876145714 \t 0.22844984637\n",
      "avg \t 1.0463215559 \t 0.227797611246\n",
      "cc \t 1.04636825679 \t 0.227810204594\n",
      "base 6 \t 1.04876145714 \t\t 0.22844984637\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def prepare_binary_ops(raw_data, emb_model, op='avg'):\n",
    "    x = []\n",
    "    y = []\n",
    "    for k,v in raw_data.items():\n",
    "        emb1,emb2 = emb_model[str(k[0])],emb_model[str(k[1])]\n",
    "        if op == 'sub':\n",
    "            x.append(emb1-emb2)\n",
    "        elif op == 'abs':\n",
    "            x.append(abs(emb1-emb2))\n",
    "        elif op == 'avg':\n",
    "            x.append((emb1+emb2)/2)\n",
    "        elif op == 'had':\n",
    "            x.append(emb1*emb2)\n",
    "        elif op == 'cc':\n",
    "            x.append(np.concatenate((emb1,emb2)))\n",
    "        elif op == 'dist':\n",
    "            x.append([euclidean(emb1,emb2)])\n",
    "        elif op == 'smax':\n",
    "            x.append([expit(np.dot(emb1,emb2))])\n",
    "        else:\n",
    "            x.append([np.dot(emb1,emb2)])\n",
    "        y.append(v)\n",
    "    return x,y      \n",
    "\n",
    "# loop over datasets (facebook, youtube)\n",
    "for d in ['facebook','youtube']:\n",
    "    print('### ' + d + '###')\n",
    "    baseline_length = 2\n",
    "    for algo in algos + ['random']:\n",
    "        print('## ' + algo + '##')\n",
    "        print('\\t MAE \\t\\t\\t MRE')\n",
    "        train_t = pickle.load(open('./data/train_' + d + '.pkl','rb'))\n",
    "        test_t = pickle.load(open('./data/test_' + d + '.pkl','rb'))\n",
    "        model = Word2Vec.load_word2vec_format(emb_path + d + '.' + algo + '.wv',binary=False)\n",
    "        for operator in ['abs', 'had','sub', 'avg', 'cc']:\n",
    "            train_x,train_y = prepare_binary_ops(train_t, model, op=operator)\n",
    "            test_x, test_y = prepare_binary_ops(test_t, model, op=operator)  \n",
    "\n",
    "            clf = LinearRegression()\n",
    "            clf.fit(train_x, train_y)\n",
    "            pred = np.array([round(p) for p in clf.predict(test_x)])\n",
    "            test_y = np.array(test_y)\n",
    "    \n",
    "            mre = np.mean(np.abs(pred-test_y)/test_y)\n",
    "            print(operator, '\\t', mean_absolute_error(test_y,pred),'\\t',mre)\n",
    "        baseline = np.ones_like(pred)*baseline_length\n",
    "        mre = np.mean(np.abs(baseline-test_y)/test_y)\n",
    "        print('base',baseline_length, '\\t', mean_absolute_error(test_y,baseline),'\\t\\t',mre)\n",
    "        baseline_length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
